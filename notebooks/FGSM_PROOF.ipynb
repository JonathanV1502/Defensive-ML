{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdqw7AhJ-jxQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm3BWtj8-rys",
        "outputId": "f1dc9799-22d4-46fb-c513-a2df324a3093"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the transformation to apply to the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1)),\n",
        "    #transforms.Lambda(lambda x: x/255.0),\n",
        "])\n"
      ],
      "metadata": {
        "id": "YvZuZ7lv-uN5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Filter out images for digits 1 and 9\n",
        "idx_1 = train_dataset.targets == 1\n",
        "idx_9 = train_dataset.targets == 9\n",
        "idx = idx_1 | idx_9\n",
        "train_dataset.targets = train_dataset.targets[idx]\n",
        "train_dataset.data = train_dataset.data[idx]\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Filter out images for digits 1 and 9\n",
        "idx_1 = test_dataset.targets == 1\n",
        "idx_9 = test_dataset.targets == 9\n",
        "idx = idx_1 | idx_9\n",
        "test_dataset.targets = test_dataset.targets[idx]\n",
        "test_dataset.data = test_dataset.data[idx]\n",
        "\n",
        "train_dataset.targets = torch.where(train_dataset.targets == 1, torch.tensor(1), torch.tensor(0))\n",
        "test_dataset.targets = torch.where(test_dataset.targets == 1, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "rnt6blAS-yVT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.data.max(), test_dataset.data.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkBFxkOcGlXJ",
        "outputId": "45309e28-a1fe-4e43-ee96-d6b403485631"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(255, dtype=torch.uint8), tensor(0, dtype=torch.uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    # Print the shape of the data\n",
        "    print(\"Shape of images:\", images.shape)\n",
        "    print(\"Shape of labels:\", labels.shape)\n",
        "    break  # Exit the loop after the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30vH-h4o---0",
        "outputId": "be455335-7aa7-47a5-f28e-ed339c13b74d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of images: torch.Size([64, 784])\n",
            "Shape of labels: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.max(),images.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFFUH4kDHidF",
        "outputId": "25c8a160-a720-4617-9238-fb5730a61776"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.targets.sum()/ len(train_dataset.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1lzgZVM_Cgj",
        "outputId": "7e0f993f-e20d-4a27-de43-51089d6827f6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5312)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMpzC2V8_KMQ",
        "outputId": "ded2637e-4a21-4c0d-a283-af083407e72c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0,  ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "import torch.nn.functional as F\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)  # 10 output classes for digits 0-9\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MLP()\n",
        "model.to(device)\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n"
      ],
      "metadata": {
        "id": "A3HYPut2_NqY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs.flatten(), targets.type(torch.float))\n",
        "        total_loss += loss.item()\n",
        "        # Calculate accuracy\n",
        "        predicted = outputs.int().flatten()\n",
        "        total_correct += (predicted == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        #if batch_idx % 100 == 0:\n",
        "            #print(f'Epoch {epoch+1}/{num_epochs}, Step {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_acc = total_correct / total_samples\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqc0FRY-_RR8",
        "outputId": "d59c9835-80e7-41fc-a1de-e890ebd154bf"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 5.7714584730440256e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 2/30, Loss: 5.087529689270019e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 3/30, Loss: 4.481449414395199e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 4/30, Loss: 3.932458813378552e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 5/30, Loss: 3.3889907962380724e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 6/30, Loss: 2.9243880812163796e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 7/30, Loss: 2.529993991953403e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 8/30, Loss: 2.2078260238774877e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 9/30, Loss: 1.9067888173897793e-08, Accuracy: 0.9991332440312032\n",
            "Epoch 10/30, Loss: 1.6451158016072983e-08, Accuracy: 0.9992908360255299\n",
            "Epoch 11/30, Loss: 1.4314203501939266e-08, Accuracy: 0.9992908360255299\n",
            "Epoch 12/30, Loss: 1.2460824324430763e-08, Accuracy: 0.9993696320226932\n",
            "Epoch 13/30, Loss: 1.0843256403168036e-08, Accuracy: 0.9993696320226932\n",
            "Epoch 14/30, Loss: 9.495342448375848e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 15/30, Loss: 8.235042533328737e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 16/30, Loss: 7.1926819647737955e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 17/30, Loss: 6.2210229792128294e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 18/30, Loss: 5.3436107818902194e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 19/30, Loss: 4.574243047708867e-09, Accuracy: 0.9994484280198566\n",
            "Epoch 20/30, Loss: 3.927856445866014e-09, Accuracy: 0.99952722401702\n",
            "Epoch 21/30, Loss: 3.3713631107251973e-09, Accuracy: 0.99952722401702\n",
            "Epoch 22/30, Loss: 2.9096280158566766e-09, Accuracy: 0.99952722401702\n",
            "Epoch 23/30, Loss: 2.491858560644096e-09, Accuracy: 0.99952722401702\n",
            "Epoch 24/30, Loss: 2.1571008319926224e-09, Accuracy: 0.99952722401702\n",
            "Epoch 25/30, Loss: 1.8736424068746663e-09, Accuracy: 0.99952722401702\n",
            "Epoch 26/30, Loss: 1.6286665019737808e-09, Accuracy: 0.99952722401702\n",
            "Epoch 27/30, Loss: 1.4119954625676293e-09, Accuracy: 0.99952722401702\n",
            "Epoch 28/30, Loss: 1.2457962301586989e-09, Accuracy: 0.99952722401702\n",
            "Epoch 29/30, Loss: 1.1156683596978825e-09, Accuracy: 0.99952722401702\n",
            "Epoch 30/30, Loss: 9.836709705607335e-10, Accuracy: 0.99952722401702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        outputs = model(data)\n",
        "        predicted = outputs.int().flatten()\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "        #correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test set: {correct/total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ1xYcJz_Yz8",
        "outputId": "6d3f8fc5-5807-4d19-a87e-abe3840dd8d3"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.9976679104477612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    Function to perform the fast gradient sign method (FGSM) attack.\n",
        "    :param image: input image\n",
        "    :param epsilon: small constant for perturbation\n",
        "    :param data_grad: gradient of the data\n",
        "    :return: perturbed image\n",
        "    \"\"\"\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "bLVVRp0JBjgq"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_model_with_fgsm(model, device, test_loader, epsilon):\n",
        "    \"\"\"\n",
        "    Function to test the model using adversarial examples created with FGSM.\n",
        "    :param model: the neural network\n",
        "    :param device: device to perform computation\n",
        "    :param test_loader: data loader for the test set\n",
        "    :param epsilon: epsilon for FGSM attack\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "    criterion = nn.BCELoss()  # Initialize BCE Loss\n",
        "\n",
        "    # Loop over all examples in the test set\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        target = target.float()  # Ensure target is float\n",
        "        data.requires_grad = True  # Enable gradient computation for the data\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Make sure the output and target have the same dimensions\n",
        "        if output.dim() == 2 and output.shape[1] == 1:\n",
        "            target = target.unsqueeze(1)\n",
        "            print(\"target\")\n",
        "            print(target.shape)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        loss = criterion(output.flatten(), target)  # Calculate the binary cross entropy loss\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of the model in the backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Check if gradients are available\n",
        "        if data.grad is not None:\n",
        "            data_grad = data.grad.data\n",
        "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "            #print(f\"PD: {perturbed_data.shape }\")\n",
        "        else:\n",
        "            print(\"Gradient not available, skipping adversarial example generation.\")\n",
        "            continue  # Skip the rest of this loop iteration if gradients are not available\n",
        "\n",
        "        # Re-classify the perturbed data\n",
        "        output = model(perturbed_data)\n",
        "        #print(f\"OO: {output.shape}\")\n",
        "        final_pred =  output.int().flatten() #binarizing it\n",
        "\n",
        "\n",
        "        # Update the correct count\n",
        "        #print(final_pred)\n",
        "        #print(target)\n",
        "        correct += (final_pred == target).sum().item()  # Ensure dimension match for comparison\n",
        "        #print(f\"before{perturbed_data}\")\n",
        "        # Collect adversarial examples for visualization if needed\n",
        "        if len(adv_examples) < 5:\n",
        "            #print(f\"before adv_ex {adv_ex}\")\n",
        "            #adv_ex1 = perturbed_data[0].squeeze().detach().cpu().numpy()\n",
        "            #print(f\"after adv_ex {adv_ex}\")\n",
        "            adv_ex = perturbed_data[0].detach().cpu().numpy()\n",
        "\n",
        "            #adv_examples.append((output[0].item(), final_pred[0].item(), adv_ex))\n",
        "            adv_examples.append((target[0].item(), final_pred[0].item(), adv_ex))\n",
        "            #print(perturbed_data)\n",
        "            #print(adv_examples)\n",
        "\n",
        "    # Calculate the final accuracy\n",
        "    final_acc = correct / float(len(test_loader.dataset))\n",
        "    print(f'Epsilon: {epsilon}, Test Accuracy = {final_acc * 100:.2f}%')\n",
        "\n",
        "    return final_acc, adv_examples\n"
      ],
      "metadata": {
        "id": "WknRc4arB3So"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "# Example of testing the model with an epsilon value\n",
        "epsilon = 0.9  # Small enough to be imperceptible, large enough to cause misclassification\n",
        "test_accuracy, adversarial_examples = test_model_with_fgsm(model, device, test_loader, epsilon)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "nbo8gPCECHRR",
        "outputId": "1c9ee2c8-f134-43e9-d331-eaee2db0657d"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ellipsis' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-93e2445f1fe1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model in evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Example of testing the model with an epsilon value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m  \u001b[0;31m# Small enough to be imperceptible, large enough to cause misclassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred, adv_image = adversarial_examples[0]\n",
        "y_true, y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H59hVJRQPTIZ",
        "outputId": "f99b174e-1a93-4229-c1bf-f4d5f0b9c7c7"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(adv_image.reshape(28,28));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "0dtdV6E-Z4MJ",
        "outputId": "35317509-ab72-4236-d970-be743633b7d0"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQ3WAcYXaJjO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}